{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04550dc8-da2d-4e06-bdff-ead2c7c4b4f8",
   "metadata": {},
   "source": [
    "# Usage of Datatonic Pipeline Components in Vertex End to End Samples\n",
    "\n",
    "Datatonic pipeline components (DTPC) is a library of vertex components that simplifies some advanced machine learning operations. They are designed to work cleanly with the Vertex End to End samples, and any other VertexAI pipelines.\n",
    "\n",
    "This notebook demonstrates how easy it is to use one such component from the DTPC library; \n",
    "\n",
    "\n",
    "## Dependencies\n",
    "Firstly, lets install the latest version of the datatonic-pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c600a7a-b0be-4747-9adc-778929574591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dtpc\n",
    "!pip install --upgrade datatonic-pipeline-components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f304e65d-64d2-4ea3-8b65-113fa202e872",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "Now we will create a new pipeline in the end-to-end samples. It'll be a simple pipeline for the purposes of demonstrating the usage of the DTPC library.\n",
    "\n",
    "Firstly, we copy the existing training pipeline as we'll be basing our new pipeline on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c72d47c6-0378-4c5e-9cb8-c2fc23fcb9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r ../pipelines/src/pipelines/training ../pipelines/src/pipelines/shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2d92fa-3f12-487f-b2a5-38919d23fa3c",
   "metadata": {},
   "source": [
    "Now, we'll overwrite the code in the pipeline.py with our new pipeline. This is a simple pipeline that takes in some input data, then uses the `gpt_tokenize`component on that new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28dcbf0c-9218-43ac-9dee-697912730084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../pipelines/src/pipelines/shap/pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../pipelines/src/pipelines/shap/pipeline.py\n",
    "\n",
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "from google_cloud_pipeline_components.v1.bigquery import BigqueryQueryJobOp\n",
    "from kfp import dsl\n",
    "from kfp.dsl import Dataset, Input, Metrics, Model, Output, OutputPath\n",
    "from pipelines import generate_query\n",
    "from bigquery_components import extract_bq_to_dataset\n",
    "from vertex_components import upload_model\n",
    "\n",
    "import datatonic_pipeline_components as dtpc\n",
    "\n",
    "CONTAINER_IMAGE_REGISTRY = os.environ[\"CONTAINER_IMAGE_REGISTRY\"]\n",
    "RESOURCE_SUFFIX = os.environ.get(\"RESOURCE_SUFFIX\", \"default\")\n",
    "TRAINING_IMAGE = f\"{CONTAINER_IMAGE_REGISTRY}/training:{RESOURCE_SUFFIX}\"\n",
    "SERVING_IMAGE = f\"{CONTAINER_IMAGE_REGISTRY}/serving:{RESOURCE_SUFFIX}\"\n",
    "\n",
    "\n",
    "@dsl.container_component\n",
    "def train(\n",
    "    train_data: Input[Dataset],\n",
    "    valid_data: Input[Dataset],\n",
    "    test_data: Input[Dataset],\n",
    "    model: Output[Model],\n",
    "    model_output_uri: OutputPath(str),\n",
    "    metrics: Output[Metrics],\n",
    "    hparams: dict,\n",
    "):\n",
    "    return dsl.ContainerSpec(\n",
    "        image=TRAINING_IMAGE,\n",
    "        command=[\"python\"],\n",
    "        args=[\n",
    "            \"training/train.py\",\n",
    "            \"--train-data\",\n",
    "            train_data.path,\n",
    "            \"--valid-data\",\n",
    "            valid_data.path,\n",
    "            \"--test-data\",\n",
    "            test_data.path,\n",
    "            \"--model\",\n",
    "            model.path,\n",
    "            \"--model-output-uri\",\n",
    "            model_output_uri,\n",
    "            \"--metrics\",\n",
    "            metrics.path,\n",
    "            \"--hparams\",\n",
    "            hparams,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "@dsl.pipeline(name=\"xgboost-train-pipeline\")\n",
    "def pipeline(\n",
    "    project_id: str = os.environ.get(\"VERTEX_PROJECT_ID\"),\n",
    "    project_location: str = os.environ.get(\"VERTEX_LOCATION\"),\n",
    "    ingestion_project_id: str = os.environ.get(\"VERTEX_PROJECT_ID\"),\n",
    "    model_name: str = \"simple_xgboost\",\n",
    "    dataset_id: str = \"preprocessing\",\n",
    "    dataset_location: str = os.environ.get(\"VERTEX_LOCATION\"),\n",
    "    ingestion_dataset_id: str = \"chicago_taxi_trips\",\n",
    "    timestamp: str = \"2022-12-01 00:00:00\",\n",
    "    resource_suffix: str = os.environ.get(\"RESOURCE_SUFFIX\"),\n",
    "    test_dataset_uri: str = \"\",\n",
    "):\n",
    "    # Create variables to ensure the same arguments are passed\n",
    "    # into different components of the pipeline\n",
    "    label_column_name = \"total_fare\"\n",
    "    time_column = \"trip_start_timestamp\"\n",
    "    ingestion_table = \"taxi_trips\"\n",
    "    table_suffix = f\"_xgb_training_{resource_suffix}\"  # suffix to table names\n",
    "    ingested_table = \"ingested_data\" + table_suffix\n",
    "    preprocessed_table = \"preprocessed_data\" + table_suffix\n",
    "    train_table = \"train_data\" + table_suffix\n",
    "    valid_table = \"valid_data\" + table_suffix\n",
    "    test_table = \"test_data\" + table_suffix\n",
    "    primary_metric = \"rootMeanSquaredError\"\n",
    "    hparams = dict(\n",
    "        n_estimators=200,\n",
    "        early_stopping_rounds=10,\n",
    "        objective=\"reg:squarederror\",\n",
    "        booster=\"gbtree\",\n",
    "        learning_rate=0.3,\n",
    "        min_split_loss=0,\n",
    "        max_depth=6,\n",
    "        label=label_column_name,\n",
    "    )\n",
    "\n",
    "    # generate sql queries which are used in ingestion and preprocessing\n",
    "    # operations\n",
    "\n",
    "    queries_folder = pathlib.Path(__file__).parent / \"queries\"\n",
    "\n",
    "    preprocessing_query = generate_query(\n",
    "        queries_folder / \"preprocessing.sql\",\n",
    "        source_dataset=f\"{ingestion_project_id}.{ingestion_dataset_id}\",\n",
    "        source_table=ingestion_table,\n",
    "        preprocessing_dataset=f\"{ingestion_project_id}.{dataset_id}\",\n",
    "        ingested_table=ingested_table,\n",
    "        dataset_region=project_location,\n",
    "        filter_column=time_column,\n",
    "        target_column=label_column_name,\n",
    "        filter_start_value=timestamp,\n",
    "        train_table=train_table,\n",
    "        validation_table=valid_table,\n",
    "        test_table=test_table,\n",
    "    )\n",
    "\n",
    "    preprocessing = (\n",
    "        BigqueryQueryJobOp(\n",
    "            project=project_id,\n",
    "            location=dataset_location,\n",
    "            query=preprocessing_query,\n",
    "        )\n",
    "        .set_caching_options(False)\n",
    "        .set_display_name(\"Ingest & preprocess data\")\n",
    "    )\n",
    "\n",
    "    # data extraction to gcs\n",
    "\n",
    "    train_dataset = (\n",
    "        extract_bq_to_dataset(\n",
    "            bq_client_project_id=project_id,\n",
    "            source_project_id=project_id,\n",
    "            dataset_id=dataset_id,\n",
    "            table_name=train_table,\n",
    "            dataset_location=dataset_location,\n",
    "        )\n",
    "        .after(preprocessing)\n",
    "        .set_display_name(\"Extract train data\")\n",
    "        .set_caching_options(False)\n",
    "    ).outputs[\"dataset\"]\n",
    "    valid_dataset = (\n",
    "        extract_bq_to_dataset(\n",
    "            bq_client_project_id=project_id,\n",
    "            source_project_id=project_id,\n",
    "            dataset_id=dataset_id,\n",
    "            table_name=valid_table,\n",
    "            dataset_location=dataset_location,\n",
    "        )\n",
    "        .after(preprocessing)\n",
    "        .set_display_name(\"Extract validation data\")\n",
    "        .set_caching_options(False)\n",
    "    ).outputs[\"dataset\"]\n",
    "    test_dataset = (\n",
    "        extract_bq_to_dataset(\n",
    "            bq_client_project_id=project_id,\n",
    "            source_project_id=project_id,\n",
    "            dataset_id=dataset_id,\n",
    "            table_name=test_table,\n",
    "            dataset_location=dataset_location,\n",
    "            destination_gcs_uri=test_dataset_uri,\n",
    "        )\n",
    "        .after(preprocessing)\n",
    "        .set_display_name(\"Extract test data\")\n",
    "        .set_caching_options(False)\n",
    "    ).outputs[\"dataset\"]\n",
    "\n",
    "    train_model = train(\n",
    "        train_data=train_dataset,\n",
    "        valid_data=valid_dataset,\n",
    "        test_data=test_dataset,\n",
    "        hparams=hparams,\n",
    "    ).set_display_name(\"Train model\")\n",
    "\n",
    "    shap_op = dtpc.xgboost_shap_gpu(input_data=train_dataset, model=train_model.outputs[\"model\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31378ae5-0cbf-4440-b3f7-8eafd78eb7c9",
   "metadata": {},
   "source": [
    "Now that we've created our new pipeline, we simply need to use the end-to-end-samples' make file to compile and execute it! At the bottom of the output for this step you'll see a link to the pipeline as it executes in your Google Cloud environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4dc17382-37eb-410d-91a1-50aaa9effd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration file exists at /Users/jackgammon/Library/Preferences/pypoetry, reusing this directory.\n",
      "\n",
      "Consider moving TOML configuration files to /Users/jackgammon/Library/Application Support/pypoetry, as support for the legacy directory will be removed in an upcoming release.\n",
      "'PipelineArtifactChannel' object is not subscriptable\n",
      "make[1]: *** [compile] Error 1\n",
      "Configuration file exists at /Users/jackgammon/Library/Preferences/pypoetry, reusing this directory.\n",
      "\n",
      "Consider moving TOML configuration files to /Users/jackgammon/Library/Application Support/pypoetry, as support for the legacy directory will be removed in an upcoming release.\n",
      "'PipelineArtifactChannel' object is not subscriptable\n",
      "make[2]: *** [compile] Error 1\n",
      "make[1]: *** [run] Error 2\n"
     ]
    }
   ],
   "source": [
    "!cd ../ && make compile pipeline=shap\n",
    "!cd ../ && make run pipeline=shap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
