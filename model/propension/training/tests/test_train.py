import numpy as np

from sklearn.metrics import roc_auc_score
from transformers import create_model


def test_model_reproducibility(synthetic_data):
    """
    Test the reproducibility of the model generated by the create_model function.
    """

    X_train, X_test, y_train, y_test = synthetic_data
    hyperparameters = {
        "n_estimators": 1500,
        "objective": "binary",
        "learning_rate": 0.01,
        "max_depth": 3,
    }
    model = create_model(hyperparameters)
    model.fit(X_train, y_train)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    auc = roc_auc_score(y_test, y_pred_proba)

    # Perform multiple runs to check reproducibility
    num_runs = 5
    for i in range(num_runs):
        model = create_model(hyperparameters)
        model.fit(X_train, y_train)
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        new_auc = roc_auc_score(y_test, y_pred_proba)
        assert np.isclose(new_auc, auc, atol=1e-7), "Model outputs differ between runs."
